{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Setting up some LaTeX stuff.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage{amsmath, amssymb, bm, dsfont, xparse, listings, mathtools, mathabx}\n",
    "\\usepackage[mathscr]{euscript}\n",
    "\\newenvironment{amatrix31}{\\left(\\begin{array}{ccc|c}}{\\end{array}\\right)}\n",
    "\\newenvironment{amatrix33}{\\left(\\begin{array}{ccc|ccc}}{\\end{array}\\right)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving systems of linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At it's core, linear algebra is just an extension of the most basic part of high school algebra: solving linear equations; e.g. performing linear manipulations (adding, subtracting, multiplying, dividing) to both sides of an equation to isolate $x$. However, linear algebra is the study of the techniques used to efficiently solve systems of multiple linear equations. E.g.\n",
    "\n",
    "$$\\begin{align*}\n",
    "3x + 2y - 2z &= 5 \\\\\n",
    "1x + 0y + 5z &= 0 \\\\\n",
    "2x + 1y + 1z &= 4\n",
    "\\end{align*}$$\n",
    "\n",
    "One can solve this system by hand quite easily by combining the equations (adding multiples of various equations to one another) until one of the variables is isolated and can be solved. Then, by backtracking, it is possible to determine all of the values.\n",
    "\n",
    "This algorithmic process is, unbelievably, almost optimal for solving such systems. It is largely akin to Gaussian elimination which is the most common way to approach these problems. To formalize this process, though, we shall need to work in the world of matrices. We start by transcribing the above problem into matrix form.\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "4\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The parallels should be obvious, but to make sure we are fully on the same page, it's important to remember how matrix multiplication works. To simplify notation, we'll first look at how to multiply a row vector by a column vector. Put simply, the result is just the sum of the pairwise products of the elements in the vectors.\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "a & b & c\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "d \\\\\n",
    "e \\\\\n",
    "f\n",
    "\\end{pmatrix} = ad + be + df$$\n",
    "\n",
    "The above sum only makes sense if the number of columns in the first vector is equal to the number of rows in the second, so the product of a row vector and a column vector is not well-defined when this isn't the case. It follow from this that, unless both vectors have only a single element, the order of multiplication cannot be swapped. In general, the order of matrix multiplication cannot be swapped (a property called non-commutativity).\n",
    "\n",
    "Now, to extrapolate to multiplying a pair of matrices, we will introduce a bit of coding-style notation. Given a matrix, $A$, we let $A[i,j]$ be the element in the $i$th row and $j$th column. Furthermore, we let $A[i,:]$ denote the entire $i$th row of $A$ (taken as a row vector) and we let $A[:,j]$ denote the entire $j$th column of $A$ (taken as a column vector). This is how rows and columns are accessed in numpy and, in general, it is a nice way to simplify these things. Adding on to this, we say that $A$ is an $r_A \\times c_A$ matrix if $A$ has $r_A$ rows and $c_A$ columns; these are the dimensions of $A$. With this in mind and with our knowledge of multiplying row vectors, we can define matrix multiplication as follows.\n",
    "\n",
    "$$(AB)[i,j] = A[i,:] B[:,j]$$\n",
    "\n",
    "In words, the entry in the $i$th row and $j$th column of $AB$ is given by the product of the $i$th row of $A$ together with the $j$th column of $B$. So what does this tell us about the necessary dimensions of $A$, $B$, and $AB$? Well, from our vector multiplication example, we know that each row vector, $A[i,:]$, has as many columns as each column vector, $B[:,j]$, has rows; i.e. the number of columns in $A$ must equal the number of rows in $B$. Moreover, $AB$ has as many rows as $A$ has (since $i$ ranges over these values) and as many columns as $B$ has (since $j$ ranges over these values). In other words, writing out the dimensions of these matrices, we get:\n",
    "\n",
    "$$\n",
    "(r_A \\times c_A)(r_B \\times c_B) \\mapsto r_A \\times c_B\\ \\textrm{ if and only if } c_A = r_B\n",
    "$$\n",
    "\n",
    "With matrix multiplication defined, we can finally see why our transcription of the original system of equations into matrix form works. Writing out the product between the $3 \\times 3$ matrix and the $3 \\times 1$ vector, we get:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "3x + 2y - 2z \\\\\n",
    "1x + 0y + 5z \\\\\n",
    "2x + 1y + 1z\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "At this point, we have simply transcribed our problem and gained nothing aside from writing the letters $x$, $y$, and $z$ two fewer times each. However, not only do matrices provide a nice notation for these types of systems, but they also provide insight into alternative methods for solving these systems. Most importantly, Gaussian elimination (the analogue for the straightforward algorithmic method described at the outset) and inverse matrices. We won't dwell too much on these methods or the algorithms for implmenting them, but it's important to understand some of the basic operations and to be familiar with manipulating matrices. For this reason, we'll start with Gaussian elimination since it is likely the more intuitive of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian elimination relies on manipulating the rows of our matrix according to a set of predetermined rules. In each step, we may:\n",
    "1. Swap the position of two rows\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix} \\xrightarrow{r_1,r_2 = r_2,r_1} \\begin{pmatrix}\n",
    "1 & 0 & 5 \\\\\n",
    "3 & 2 & -2 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "2. Multiply a row by a non-zero scalar\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix} \\xrightarrow{r_1 *= \\frac{1}{3}} \\begin{pmatrix}\n",
    "1 & \\frac{2}{3} & \\frac{-2}{3} \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "3. Add to one row a scalar multiple of another\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix} \\xrightarrow{r_3 += -2 r_2} \\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "0 & 1 & -9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The ultimate goal of these operations is to reduce our matrix to reduced row echelon form (RREF). This is a form where:\n",
    "- all rows consisting of only 0s are at the bottom\n",
    "- the leading non-zero entry (also called the pivot) in every row is a 1\n",
    "- the pivot of every row is strictly to the right of the pivot of the row above it\n",
    "- each column containing a pivot has 0s in all other entries\n",
    "\n",
    "For example, the following matrix is in RREF:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & a_1 & 0 & b_1 \\\\\n",
    "0 & 1 & a_2 & 0 & b_2 \\\\\n",
    "0 & 0 & 0 & 1 & b_3 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now, why are these operations allowed and why is RREF so special? These operations are allowed since they mimic the ways that we can deal with simultaneous linear equations by combining them. As far as RREF is concerned, RREF is the process by which we isolate variables and solve for them. If one of the rows has a single non-zero entry (the pivot which is 1), then we have successfuly isolated the variable and can solve for it. To use this technique, we'll perform Gaussian elimination on a matrix onto which we have augmented the target vector. I.e. we do the following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{amatrix31}\n",
    "3 & 2 & -2 & 5 \\\\\n",
    "1 & 0 & 5 & 0 \\\\\n",
    "2 & 1 & 1 & 4\n",
    "\\end{amatrix31} &\\xrightarrow{r_1,r_2 = r_2,r_1} \\begin{amatrix31}\n",
    "1 & 0 & 5 & 0 \\\\\n",
    "3 & 2 & -2 & 5 \\\\\n",
    "2 & 1 & 1 & 4\n",
    "\\end{amatrix31} \\\\\n",
    "&\\xrightarrow{r_2 += -3r_1} \\begin{amatrix31}\n",
    "1 & 0 & 5 & 0 \\\\\n",
    "0 & 2 & -17 & 5 \\\\\n",
    "2 & 1 & 1 & 4\n",
    "\\end{amatrix31} \\\\\n",
    "&\\xrightarrow{r_3 += -2r_1} \\begin{amatrix31}\n",
    "1 & 0 & 5 & 0 \\\\\n",
    "0 & 2 & -17 & 5 \\\\\n",
    "0 & 1 & -9 & 4\n",
    "\\end{amatrix31} \\\\\n",
    "&\\xrightarrow{r_2,r_3 = r_3,r_2} \\begin{amatrix31}\n",
    "1 & 0 & 5 & 0 \\\\\n",
    "0 & 1 & -9 & 4 \\\\\n",
    "0 & 2 & -17 & 5 \n",
    "\\end{amatrix31} \\\\\n",
    "&\\xrightarrow{r_3 += -2r_2} \\begin{amatrix31}\n",
    "1 & 0 & 5 & 0 \\\\\n",
    "0 & 1 & -9 & 4 \\\\\n",
    "0 & 0 & 1 & -3 \n",
    "\\end{amatrix31} \\\\\n",
    "&\\xrightarrow{r_1 += -5r_3} \\begin{amatrix31}\n",
    "1 & 0 & 0 & 15 \\\\\n",
    "0 & 1 & -9 & 4 \\\\\n",
    "0 & 0 & 1 & -3 \n",
    "\\end{amatrix31} \\\\\n",
    "&\\xrightarrow{r_2 += 9r_3} \\begin{amatrix31}\n",
    "1 & 0 & 0 & 15 \\\\\n",
    "0 & 1 & 0 & -23 \\\\\n",
    "0 & 0 & 1 & -3 \n",
    "\\end{amatrix31}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As it turns out, the row operations which we are allowed to perform preserve the linear equation. I.e.\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "4\n",
    "\\end{pmatrix}\\ \\ \\textrm{ became }\\ \\ \\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "15 \\\\\n",
    "-23 \\\\\n",
    "-3\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The matrix on the left is called the identity matrix since it is the \"one\" element of the ring of matrices. This just means that multiplying by it does not change anything. This means our final solution is:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "15 \\\\\n",
    "-23 \\\\\n",
    "-3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now that we have solved the question once, what would we do if we were given a different target vector? We could repeat the above process, but it seems like a huge waste of time to do so. Instead, if we had taken the time to remember our row operations, we could go through the process much more quickly. This leads us to the next method: inverse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the inverse matrix can be done in several ways, but the method most in line with what we have already done is to go through Gaussian elimination with an augmented identity matrix at the start. This results in the following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{amatrix33}\n",
    "3 & 2 & -2 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 5 & 0 & 1 & 0 \\\\\n",
    "2 & 1 & 1 & 0 & 0 & 1\n",
    "\\end{amatrix33} &\\xrightarrow{r_1,r_2 = r_2,r_1} \\begin{amatrix33}\n",
    "1 & 0 & 5 & 0 & 1 & 0 \\\\\n",
    "3 & 2 & -2 & 1 & 0 & 0 \\\\\n",
    "2 & 1 & 1 & 0 & 0 & 1\n",
    "\\end{amatrix33} \\\\\n",
    "&\\xrightarrow{r_2 += -3r_1} \\begin{amatrix33}\n",
    "1 & 0 & 5 & 0 & 1 & 0 \\\\\n",
    "0 & 2 & -17 & 1 & -3 & 0 \\\\\n",
    "2 & 1 & 1 & 0 & 0 & 0\n",
    "\\end{amatrix33} \\\\\n",
    "&\\xrightarrow{r_3 += -2r_1} \\begin{amatrix33}\n",
    "1 & 0 & 5 & 0 & 1 & 0 \\\\\n",
    "0 & 2 & -17 & 1 & -3 & 0 \\\\\n",
    "0 & 1 & -9 & 0 & -2 & 1\n",
    "\\end{amatrix33} \\\\\n",
    "&\\xrightarrow{r_2,r_3 = r_3,r_2} \\begin{amatrix33}\n",
    "1 & 0 & 5 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & -9 & 0 & -2 & 1 \\\\\n",
    "0 & 2 & -17 & 1 & -3 & 0 \n",
    "\\end{amatrix33} \\\\\n",
    "&\\xrightarrow{r_3 += -2r_2} \\begin{amatrix33}\n",
    "1 & 0 & 5 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & -9 & 0 & -2 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 1 & -2 \n",
    "\\end{amatrix33} \\\\\n",
    "&\\xrightarrow{r_1 += -5r_3} \\begin{amatrix33}\n",
    "1 & 0 & 0 & -5 & -4 & 10 \\\\\n",
    "0 & 1 & -9 & 0 & -2 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 1 & -2 \n",
    "\\end{amatrix33} \\\\\n",
    "&\\xrightarrow{r_2 += 9r_3} \\begin{amatrix33}\n",
    "1 & 0 & 0 & -5 & -4 & 10 \\\\\n",
    "0 & 1 & 0 & 9 & 7 & -17 \\\\\n",
    "0 & 0 & 1 & 1 & 1 & -2 \n",
    "\\end{amatrix33}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In this way, we have essentially kept track of all of our row operations, and the resulting augmented matrix is called the inverse matrix. Given a matrix, $A$, we let $A^{-1}$ denote it's inverse, which is the unique matrix such that $AA^{-1} = I = A^{-1}A$, where $I$ is the identity. What this means is that we can multiply both sides of our linear equation by this inverse matrix to get:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} &= \\begin{pmatrix}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "4\n",
    "\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "-5 & -4 & 10 \\\\\n",
    "9 & 7 & -17 \\\\\n",
    "1 & 1 & -2\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "3 & 2 & -2 \\\\\n",
    "1 & 0 & 5 \\\\\n",
    "2 & 1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} &= \n",
    "\\begin{pmatrix}\n",
    "-5 & -4 & 10 \\\\\n",
    "9 & 7 & -17 \\\\\n",
    "1 & 1 & -2\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "4\n",
    "\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} &= \n",
    "\\begin{pmatrix}\n",
    "-5 & -4 & 10 \\\\\n",
    "9 & 7 & -17 \\\\\n",
    "1 & 1 & -2\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "4\n",
    "\\end{pmatrix} \\\\\n",
    "\\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{pmatrix} &= \n",
    "\\begin{pmatrix}\n",
    "-5 & -4 & 10 \\\\\n",
    "9 & 7 & -17 \\\\\n",
    "1 & 1 & -2\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "4\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As we see, once we have found the inverse matrix, we can now solve for $x$, $y$, and $z$ by performing a single matrix multiplication.\n",
    "\n",
    "Below we introduce the very basic capabilities of numpy arrays and the functions in numpy.linalg. We can input a matrix as a list of its rows. Numpy has a built-in solver for linear systems of equations, but we can also find the solution by first calculating the matrix inverse, then using .dot to multiply our inverse by the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "np.linalg.solve solves linear systems"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 15.],\n",
       "       [-23.],\n",
       "       [ -3.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "np.linalg.inv finds a matrix inverse and .dot is used for matrix multiplication"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 15.],\n",
       "       [-23.],\n",
       "       [ -3.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "from numpy.linalg import *\n",
    "\n",
    "A = np.array([[3,2,-2],[1,0,5],[2,1,1]])\n",
    "b = np.array([[5],[0],[4]])\n",
    "display(Markdown(\"np.linalg.solve solves linear systems\"))\n",
    "display(solve(A,b))\n",
    "display(Markdown(\"np.linalg.inv finds a matrix inverse and .dot is used for matrix multiplication\"))\n",
    "display(inv(A).dot(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic points before we move on:\n",
    "1. Matrices are not always invertible. A matrix admits an inverse only if its determinant (a value you can calculate from the entries of the matrix) is invertible (i.e. non-zero). If a matrix does not admit an inverse, a corresponding linear system might be unsolvable, or it might have more than one solution. Gaussian elimination will always give you every solution or will prove that no solutions exist, so it is a good backup if the inverse doesn't exist.\n",
    "2. Linear systems need not have the same number of equations as variables. Non-square matrices do not admit inverses, since the unique inverse matrix must be able to multiply on the left and the right. For non-square matrices, we can define a left-inverse or a right-inverse, if they exist, although this isn't super important. There are also things called pseudoinverses that hopefully will never come up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been working with matrices over the reals. In fact, I went through the trouble of finding a unimodular matrix (a matrix with determinant equal to $\\pm 1$) for our example so we could stick with whole numbers. However, there are plenty of options for doing linear algebra over other types of fields. This is where much of the complexity comes in. For our purposes, we shall only deal with matrices over the complex field and over the field $\\mathbb{F}_2$.\n",
    "\n",
    "Hopefully we remember enough about complex numbers that we don't need to dwell on them for long. The important fact is that $i = \\sqrt{-1}$. For every complex number, $a + bi$, we denote it's conjugate by $\\overline{a + bi} = a-bi$. This conjugate operation ends up being super useful. Adding complex numbers is done componentwise and multiplying complex numbers is essentially done by FOILing the terms, as in the following:\n",
    "$$\n",
    "(a + bi)(c + di) = ac + adi + bci + bdii = (ac-bd)+(ad+bc)i\n",
    "$$\n",
    "\n",
    "The field $\\mathbb{F}_2$ (the unique Galois field of two elements) seems a bit more esoteric, but it's actually just the field of binary numbers where addition is XOR and multiplication is AND.\n",
    "\n",
    "When we are dealing with Clifford operators acting on stabilizer states (don't worry yet what these things mean), all of our operations will be performed on symplectic matrices (it's kinda like augmenting matrices, but weirder) over $\\mathbb{F}_2$. The incredible thing about this is that the dimension of our state scales linearly with respect to the number of qubits in our simulation. This means we can efficiently simulate these types of circuits.\n",
    "\n",
    "When we are dealing with more general quantum states/operators, we will need to work over the complex field, and the dimension of our state will scale exponentially with respect to the number of qubits in our simulation. This is why quantum mechanics is not efficiently simulable (at least, to the best of everyone's knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Quantum Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
